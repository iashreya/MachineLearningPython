{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression","execution_count":69,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"pos_rev = open('../input/positive.txt', encoding='latin-1').read()\nneg_rev = open('../input/negative.txt', encoding='latin-1').read()\npos_rev = pos_rev.split('\\n')\nneg_rev = neg_rev.split('\\n')","execution_count":86,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pos_rev), len(neg_rev)","execution_count":87,"outputs":[{"output_type":"execute_result","execution_count":87,"data":{"text/plain":"(5331, 5331)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing data"},{"metadata":{},"cell_type":"markdown","source":"***Tokenizing Reviews***"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = TreebankWordTokenizer()\npos_rev_token = [tokenizer.tokenize(rev) for rev in pos_rev]\nneg_rev_token = [tokenizer.tokenize(rev) for rev in neg_rev]","execution_count":88,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Removing punctuations and Numbers***"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_rev_token = [[token for token in rev if token.isalpha()] for rev in pos_rev_token]\nneg_rev_token = [[token for token in rev if token.isalpha()] for rev in neg_rev_token]","execution_count":73,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Stemming***"},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = PorterStemmer()\npos_rev_token = [[ps.stem(token) for token in rev] for rev in pos_rev_token]\nneg_rev_token = [[ps.stem(token) for token in rev] for rev in neg_rev_token]","execution_count":22,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Removing stopwords***"},{"metadata":{"trusted":true},"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\npos_rev_token = [[token for token in rev if token not in STOPWORDS] for rev in pos_rev_token]\nneg_rev_token = [[token for token in rev if token not in STOPWORDS] for rev in neg_rev_token]","execution_count":89,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Removing words which have length < 3***"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_rev_token = [[token for token in rev if len(token)>2] for rev in pos_rev_token]\nneg_rev_token = [[token for token in rev if len(token)>2] for rev in neg_rev_token]","execution_count":90,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting data into test and train sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = pos_rev_token[0:5000]+neg_rev_token[0:5000]\ny_train = [1 for i in range(5000)]+[0 for i in range(5000)]\nx_test = pos_rev_token[5000:]+neg_rev_token[5000:]\ny_test = [1 for i in range(len(pos_rev_token)-5000)]+[0 for i in range(len(neg_rev_token)-5000)]","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(x_train), len(y_train), len(x_test), len(y_test)","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"(10000, 10000, 662, 662)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"***Shuffling the dataset***"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame({'text':x_train, 'rev':y_train})\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntest_df = pd.DataFrame({'text':x_test, 'rev':y_test})\ntest_df = test_df.sample(frac=1).reset_index(drop=True)","execution_count":93,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Tfidf Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\nx_train_tfidf = vectorizer.fit_transform([' '.join(rev) for rev in train_df['text']])\nx_test_tfidf = vectorizer.transform(' '.join(rev) for rev in test_df['text'])","execution_count":94,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{},"cell_type":"markdown","source":"***Using SVM***"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = svm.SVC(gamma='scale')\nclf.fit(x_train_tfidf, y_train)\npredictions = clf.predict(x_test_tfidf)\nprint(classification_report(predictions, y_test))\n","execution_count":95,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.48      0.51      0.49       309\n           1       0.54      0.51      0.53       353\n\n   micro avg       0.51      0.51      0.51       662\n   macro avg       0.51      0.51      0.51       662\nweighted avg       0.51      0.51      0.51       662\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"***Using Logistic Regression***"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(penalty='l2', C=1)\nclf.fit(x_train_tfidf, y_train)\npredictions = clf.predict(x_test_tfidf)\nprint(classification_report(predictions, y_test))","execution_count":96,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.47      0.50      0.49       312\n           1       0.53      0.50      0.51       350\n\n   micro avg       0.50      0.50      0.50       662\n   macro avg       0.50      0.50      0.50       662\nweighted avg       0.50      0.50      0.50       662\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"***Using Naive Bayes Classifier***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GaussianNB().fit(x_train_tfidf.toarray(), y_train)\npredictions = clf.predict(x_test_tfidf.toarray())\nprint(classification_report(predictions, y_test))","execution_count":98,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.43      0.51      0.47       275\n           1       0.60      0.51      0.55       387\n\n   micro avg       0.51      0.51      0.51       662\n   macro avg       0.51      0.51      0.51       662\nweighted avg       0.52      0.51      0.51       662\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB().fit(x_train_tfidf.toarray(), y_train)\npredictions = clf.predict(x_test_tfidf.toarray())\nprint(classification_report(predictions, y_test))","execution_count":99,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.50      0.53      0.51       314\n           1       0.55      0.53      0.54       348\n\n   micro avg       0.53      0.53      0.53       662\n   macro avg       0.53      0.53      0.53       662\nweighted avg       0.53      0.53      0.53       662\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = BernoulliNB().fit(x_train_tfidf.toarray(), y_train)\npredictions = clf.predict(x_test_tfidf.toarray())\nprint(classification_report(predictions, y_test))","execution_count":100,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.50      0.53      0.51       318\n           1       0.54      0.52      0.53       344\n\n   micro avg       0.52      0.52      0.52       662\n   macro avg       0.52      0.52      0.52       662\nweighted avg       0.52      0.52      0.52       662\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### The accuracy is very poor."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}